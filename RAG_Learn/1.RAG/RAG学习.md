## 引言

在过去几年里，大型语言模型（LLM）以其惊人的语言理解和生成能力，彻底改变了我们与技术互动的方式。从写代码、作诗到进行多轮对话，LLM 仿佛无所不能，给我们造成了一种它们 **无所不知** 的印象。然而，当我们尝试将这些强大的模型真正应用到严肃的企业场景时，其固有的 阿喀琉斯之踵 便暴露无遗。

> **“阿喀琉斯之踵”（*****\*Achilles' heel\******）**是一个源自**古希腊神话**的比喻，指的是一个人或事物**表面看似强大无敌，但却存在致命弱点**，这个弱点一旦被击中，就会导致失败或灭亡。

首先是 **知识截止（Knowledge Cutoff）** 问题。LLM 的知识来源于其训练数据，而训练是一个极其昂贵且耗时的过程。因此，任何一个模型的知识都被**冻结**在了某个特定的时间点。它不知道最近发生的新闻，不了解最新的市场动态，更无法访问实时的信息。

其次是 **模型幻觉（Hallucination）**。LLM 的 本质是基于概率生成文本，它会尽力产出听起来最连贯、最 plausible 的内容，但这并不等同于事实。当模型被问及它知识范围之外或模糊不清的问题时，它可能会一本正经地胡说八道，编造出看似合理但完全错误的答案。这种幻觉从无意义的输出到与事实的矛盾，形式多样，极大地侵蚀了用户对模型的信任，是企业级应用落地的致命障碍。

最后，也是最关键的一点，是 **私域知识的缺失（Lack of Domain-Specific & Private Data）**。任何一家企业都有其独特的产品文档、内部知识库、客户数据和业务流程。这些私有的、领域特定的知识，通用的 LLM 从未学习过，自然也无法回答与之相关的问题。让模型重新训练或进行大规模微调来学习这些知识，高昂的成本往往会让大多数公司望而却步。

为了解决这些根本性问题，一种优雅而高效的架构应运而生——**检索增强生成（Retrieval-Augmented Generation, RAG）**。你可以把它想象成给 LLM 配备了一个可随时查阅的“外脑”或“随身小百科”。RAG 架构将 LLM 强大的推理能力与外部的、私有的知识库连接起来，让模型在回答问题前，先去“查找资料”，然后依据可靠的资料来组织答案。

这种模式代表了一种应用 AI 的范式转变：我们不再强求模型本身成为一个无所不知的“知识库”，而是将其定位为一个强大的“推理引擎”。模型的任务从“记忆事实”转变为“理解和处理实时提供的事实”。这种解耦使得 AI 应用更加可靠、可扩展且易于维护，是推动 LLM 从“玩具”走向“工具”的关键一步。

<font color='red'>**RAG 的核心原理**</font>

要理解 RAG 的工作原理，最贴切的比喻莫过于一场 **开卷考试**。

- 一个 **标准的 LLM**，就像一个参加 **“闭卷考试”** 的学生。他只能依赖自己脑中已经背诵的知识来回答问题。如果遇到没复习到的知识点，就只能凭感觉猜测，甚至编造答案。
- 一个 **搭载了 RAG 的 LLM**，则像一个参加 **“开卷考试”** 的学生。他不需要记住所有细节。当遇到问题时，他会先翻阅桌上的教科书和参考资料（外部知识库），找到最相关的章节和段落，然后基于这些准确的信息，整理出逻辑清晰、内容详实的答案。

这个 **“开卷考试”** 的过程，在技术上被分解为两个核心阶段：**检索（Retrieval）**和 **增强生成（Augmented Generation）** 。

- **检索（Retrieval）**：当系统收到用户的提问后，它不会立刻把问题丢给 LLM。相反，它首先将用户的问题作为一个查询指令，在外部知识库（如公司的产品文档、数据库、知识图谱等）中进行搜索，找出与问题最相关的信息片段。
- **增强生成（Augmented Generation）**：系统将上一步检索到的相关信息，连同用户的原始问题，一起打包成一个内容更丰富、上下文更明确的“增强提示词”（Augmented Prompt）。这个增强后的提示词被发送给 LLM，并明确指示 LLM：“请根据我提供的这些背景资料来回答这个问题”。LLM 此时的角色不再是凭空创作，而是在限定的、可靠的资料范围内进行归纳、总结和生成。

RAG 的巧妙之处在于，它不仅提升了答案的准确性和时效性，还天然地解决了 LLM 的“黑箱”问题，增强了系统的**透明度和可信度**。因为 RAG 的回答是基于具体的、被检索出的文本，系统可以轻易地将这些“参考文献”展示给用户，比如通过脚注或链接的形式。用户可以追根溯源，自行验证信息的准确性。这种“有据可查”的能力，是获得用户信任、实现业务落地的基本前提。

## 一、RAG概览

RAG （检索增强生成）是企业 AI 大模型应用落地的主要应用形态之一，特别是在智能问答、报告生成、内容审核、Text2SQL、流程自动化和 AI 编程等领域大规模应用和落地 RAG 架构。

![图片](RAG学习.assets\640.gif)



从技术本质来分析，**RAG 架构设计是由两部分构成：数据工程和信息抽取**。其中数据工程是最重要的部分，数据工程的最重要的工作之一是**对文档进行分块（Chunking）**。

因为要处理的文档可能会相当大，所以第一步还包括“**分块**”，就是把一个大文件分成更小的、更容易处理的部分。

![图片](RAG学习.assets\640-1755069006307-3.png)

这个步骤非常重要，因为它确保文本能够适应嵌入模型的输入大小。

而且，它还能提高信息抽取步骤的效率和准确性，这直接影响到生成回答的质量。



## 二、文档是如何变成知识的？

### 1. RAG 系统中的*外部大脑*：文档的角色

在 RAG 架构中，文档扮演着“外部大脑”或“权威知识库”的角色。它的核心使命是为 LLM 提供一个事实的基石（grounding），当模型需要回答问题时，它不再仅仅依赖于自己内部模糊的、可能过时的记忆，而是可以查阅这些外部文档来获取最相关、最准确的信息。

这个过程具体表现为：系统将从文档中检索到的 **事实** 片段，连同用户的问题一起，打包成一个更丰富的提示（prompt）喂给 LLM。这种做法带来了两个立竿见影的好处：首先，它极大地减少了模型幻觉的概率；其次，它让模型的回答变得可以溯源，用户可以清楚地看到答案是基于哪些文档生成的，这对于需要高可信度的企业应用场景至关重要。

### 2. 知识从哪里来？五花八门的文档来源

![Embedding——从入门到生产使用| 我的学习笔记| 土猛的员外](RAG学习.assets\640-1756260846404-2.png)

企业知识的形态是复杂多样的，它们散落在不同的系统和媒介中。一个强大的 RAG 系统必须具备从这些异构源头汲取知识的能力。常见的文档来源包括：

-  **非结构化数据**：这是最常见的知识载体，如 PDF 报告、Word 文档（.docx）、技术手册（Markdown）、纯文本文档（.txt）以及海量的网页（HTML）。
- **半结构化数据**：这类数据具有一定的组织形式，例如 CSV 表格、JSON 文件、Notion 页面、Confluence 知识库等。
-  **结构化数据**：直接来源于业务系统，如从 Mysql 等关系型数据库中导出的数据，或是通过 API 接口获取的实时信息。

为了应对这种复杂性，像 `LangChain` 这样的开发框架提供了丰富的 **文档加载器（Document Loaders）** 。这些加载器就像是特制的 **数据插头** ，每一种都针对特定的数据源，能够高效地读取内容并将其转换为统一的格式。LangChain 官方支持的加载器列表非常庞大，从常见的 PDF、网页，到 Slack 聊天记录、Figma 设计文件，甚至 Bilibili 视频字幕，几乎无所不包。

这种广泛的覆盖揭示了一个深刻的现实：**对于 RAG 应用开发者而言，首要的挑战往往不是算法，而是数据集成。所谓的 “文档”，其实是对一个混乱、异构数据世界的便捷抽象。一个 RAG 项目的成败，很大程度上取决于能否打通连接到企业内部各个“知识孤岛”的数据管道**。

### 3. 核心目标：从“原始数据”到“可检索的知识单元”

数据加载进来后，我们便开启了整个流程的核心目标：将这些大小不一、格式混乱的原始数据，转化为一系列干净、语义连贯、尺寸统一的文本片段。这些片段在技术上被称为 **“块”（Chunks）** 或 **“知识单元”（Knowledge Units）**。

可以把这个过程想象成准备烹饪的食材。原始文档就像是刚从地里挖出来的土豆，上面还带着泥土，大小也不一。我们需要先把它洗干净（清洗），然后切成大小均匀的块（切分），这样才能方便后续的烹饪（向量化和检索）。这个预处理步骤是整个 RAG 流程的基石，因为这些“知识单元”的质量，将直接决定后续检索的精准度和最终生成答案的质量。



## 三、预处理：给知识“梳洗打扮”

将原始文档转化为高质量的知识单元，是 RAG 系统工程中最具挑战也最关键的一步。这个过程包含两个核心环节：拆分（Chunking）与清洗（Cleaning）。

### 1. 核心挑战：如何切分得“恰到好处”？

拆分（Chunking）面临一个经典的困境：知识单元既要足够小，以适应 Embedding 模型和 LLM 的上下文窗口限制；又要足够大，以保留完整的语义信息。

- **块太小**：如果一个块只有半句话，它就失去了上下文，语义不完整。模型很难理解它的真实含义。
- **块太大**：如果一个块包含多个不相关的主题，它内部就会充满“噪声”，稀释了核心信息，导致在检索时虽然被匹配到，但大部分内容对回答问题并无帮助，反而会干扰 LLM 的判断。

经验法则是：**如果一段文本在没有上下文的情况下，人类能够理解其含义，那么语言模型大概率也能理解**。这为我们追求“恰到好处”的切分提供了方向。



### **2.RAG 架构设计的5种分块技术剖析**

企业 RAG 架构落地，常常使用5种分块技术：**固定大小分块、语义分块、递归分块、基于文档结构分块、基于 LLM 分块**。

![图片](RAG学习.assets\640-1755071203704-6.gif)



**第一、固定大小分块（Fixed-size chunking）**

生成文本块最直观和简单的方法就是根据预先设定的字符数、单词数或 tokens，把文本切成大小一致的小段。

这是最简单直接的方法，按照固定的字符数或 Token 数来切割文本。通常会设置两个关键参数：`chunk_size`（块大小）和`chunk_overlap`（块重叠）；重叠部分的存在是为了在两个连续的块之间保留一定的上下文，防止信息在边界处被完全切断。然而，这种“一刀切”的方式非常“天真”，它完全无视文本的自然结构，很可能在句子中间、一个完整的概念描述到一半时就强行分割，破坏语义完整性。

![图片](RAG学习.assets\640-1755071253783-9.png)



**第二、语义分块（Semantic chunking）**

语义分块方法很简单。这代表了更前沿的探索方向。语义拆分不再依赖于字符或结构，而是通过计算句子之间 embedding 的相似度来决定分割点。当它检测到文本的语义主题发生了显著变化时，就在此处进行分割，从而生成语义上高度内聚的块。

![图片](RAG学习.assets\640-1755071406410-12.gif)

就是根据有意义的单元来切分文档，比如：句子、段落或者主题部分。

然后，为每个部分创建嵌入向量（一种能表示文本意义的数字）。

假设我从第一个部分和它的嵌入向量开始。

- 如果第一个部分的嵌入向量和第二个部分的嵌入向量之间的余弦相似度很高，那么这两个部分就组成一个文本块。
- 这个过程会一直继续，直到余弦相似度明显下降。
- 一旦相似度下降了，我们就开始一个新的文本块，并重复这个过程。

输出的结果可能如下：

![图片](RAG学习.assets\640-1755071415095-15.png)

和固定大小的文本块不同，这种方法保持了语言的自然流畅性，并保留了完整的思想。

因为每个文本块包含的信息更丰富，它提高了检索的准确性，这反过来又使得大语言模型（LLM）生成的回答更加连贯和相关。

一个小问题是，它依赖于一个**阈值**来确定余弦相似度是否显著下降，这个阈值可能因文档而异。

**第三、递归分块（Recursive chunking）**

递归分块方法也很容易理解。 这是目前最常用、也是最推荐的起点策略。它的高明之处在于，它试图尽可能地尊重文本的自然结构。其工作原理是，提供一个按优先级排序的分隔符列表，比如 `["\n\n", "\n", " ", ""]`。它会首先尝试用最高优先级的“换段符” (`\n\n`) 来分割文本。如果分割后的块仍然大于设定的 `chunk_size`，它就会在这些大块上，用次一级优先级的“换行符” (`\n`) 继续分割。这个过程会递归地进行下去，直到所有块都符合尺寸要求。这样做的效果是，它会优先保持段落的完整，其次是句子的完整，最后才是单词的完整。

![图片](RAG学习.assets\640-1755071422810-18.gif)

首先，根据文档中的自然分隔符来分块，比如：段落或章节。

然后，如果某个块的大小超过了我们预先设定的块大小限制，就把它再分成更小的块。如果块的大小已经在限制范围内，就不用再分了。

可能的输出结果是这样的：

![图片](RAG学习.assets\640-1755071429576-21.png)

如上所示：

- 我们首先定义了两个块（紫色的两个段落）。
- 然后，第一段被进一步分成了更小的块。

和固定大小的块相比，这种方法同样保留了语言的自然流畅性，并且保留了完整的语义。

不过，这种方法在实现和计算复杂度上会稍微复杂一些。



**第四、基于文档结构分块（Document structure-based chunking）**

基于文档结构分块是一种很直观的方法。  对于那些自身就带有明确结构的文件格式，如 Markdown 和 HTML，我们可以利用其内在结构来进行更智能的分割。例如，对于 Markdown 文件，我们可以将每个标题（如 `#` 一级标题，`##` 二级标题）作为天然的分割点。LangChain 框架中的 `MarkdownHeaderTextSplitter` 就是为此而生，它可以确保每个标题下的所有内容形成一个独立的、上下文完整的知识单元。

![图片](RAG学习.assets\640-1755071437713-24.gif)

它利用文档本身的结构，比如：**标题、章节或段落，来确定分块的边界**。

这样做的好处是，它能够保持文档结构的完整性，因为它和文档的逻辑部分是对齐的。

输出的结果可能看起来像这样：

![图片](RAG学习.assets\640-1755071448989-27.png)

话虽如此，这种方法假设文档有一个清晰的结构，但有时候可能并非如此。

而且，分出来的块长度可能会不一样，有可能超出大模型处理的字数限制。**实际落地是会把它和递归分块结合起来使用**。



**第五、基于 LLM 分块（LLM-based chunking）**

既然每种分块方法都有优点和缺点，那为什么不让大语言模型（LLM）来创建文本块呢？**智能体拆分**则更进一步，它让一个 LLM Agent 来扮演“文档分析师”的角色，由模型自己来判断如何分割才是最合理的，这模拟了人类在处理长文档时的推理过程。

![图片](RAG学习.assets\640-1755071456555-30.gif)

大语言模型可以被引导生成语义上独立且有意义的文本块。

很明显，这种方法将确保高语义准确性，因为大语言模型具备世界知识，能理解上下文和含义，这超出了上面四种方法中使用的简单启发式方法。

唯一的问题是，这是这里讨论的五种技术中计算量最大的文本块划分技术。

另外，由于大语言模型通常有一个有限的上下文窗口，这也是需要考虑的一点。

总之，每种技术都有它自己的优势和权衡，**5种分块技术总结对比如下**：

![图片](https://mmbiz.qpic.cn/mmbiz_png/9TPn66HT930YEBZLAx1mQibh8BH4ElbyUhMHCV9bMSyCo3c4T9CUuCpNRp0elX9nR7s7ugib35OibECF4cMTsF5CA/640?wx_fmt=png&from=appmsg&tp=wxpic&wxfrom=5&wx_lazy=1)

我们实际落地发现，**语义分块在很多情况下都效果很好**，你在 RAG 应用落地的时候可以首选，但是最好针对你的业务场景进行测试下。

**RAG 架构落地过程中最终选择哪种分块技术，将很大程度上取决于你的内容性质、嵌入模型的能力、计算资源等等**。



### 3. 清洗的艺术：去除噪音，保留精华

清洗的目标是去除文档中的无关信息（如广告、导航栏、脚本代码），同时保留有价值的结构和内容。

- **HTML 清洗：提取正文 vs. 保留结构**

  ​        处理网页内容时存在两种哲学。传统方法是使用 BeautifulSoup 或 Readability.js 这样的库，将 HTML 中的所有标签、脚本和样式全部剥离，只提取出“纯文本”正文。这种方法简单有效，但代价是丢失了所有版式结构信息。而前沿的研究，如 **HtmlRAG**，则提出了一个相反的观点：粗暴地剥离标签会导致严重的信息损失，例如，表格、列表、代码块会退化成一堆难以理解的混乱文本。HtmlRAG 的精细化处理流程包括：

  ​        这种精细化处理的背后，是一个至关重要的因果关系：**糟糕的解析是下游幻觉的直接源头**。如果解析过程错误地将一个表格的行列关系打乱，或者将页眉页脚的内容混入正文，那么产出的知识单元本身就是错误的“事实”。当 LLM 被喂给这些错误信息作为权威来源时，它必然会生成一个混乱、不准确甚至完全错误的答案。因此，高质量的解析是 RAG 系统对抗幻觉的第一道，也是最关键的一道防线。

  ![WX20250803-132126@2x](RAG学习.assets\640.png)

  - **清洗**：只移除真正无用的标签（如 `<script>`, `<style>`）和冗余属性。

  - **压缩**：合并多余的嵌套结构，如将 `<div><div><p>text</p></div></div>` 简化为 `<p>text</p>`。

  - **剪枝**：基于内容与查询的相关性，智能地移除整个 HTML 树中的无关分支。

- **Markdown 清洗**：重点在于保留其结构化元素。例如，一个 Markdown 表格应该被识别并作为一个整体来处理，而不是被拆散成零散的文本行，代码块和列表也应同理。

![image-20250803125556252](RAG学习.assets\640-1756262150229-6.png)

### 4. 如何处理复杂元素？

现实世界的文档远不止纯文本。如何处理表格、代码和图片，是衡量一个 RAG 系统成熟度的重要标准。下表总结了处理这些复杂元素的一些最佳实践。

**表1：复杂文档元素处理最佳实践**

| **元素类型**  | **推荐处理策略**                                             | **关键考量**                                                 |
| ------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **表格**      | 1. **整体处理**：避免将一个表格分割到多个知识单元中。 2. 结构化转换：在向量化之前，将表格转换为更清晰的 Markdown、JSON 或 CSV 格式。 3. 自然语言摘要：使用 LLM 生成一段描述表格内容的自然语言摘要。 4. 专用模型处理：TableRAG 等前沿方法建议将表格数据存入可供 SQL 查询的数据库中，以支持更复杂的推理。 | 核心目标是保护表格的行列结构，这承载了其核心语义。简单的文本提取会彻底破坏这种结构。将其转换为结构化格式是目前最稳妥的检索方案。 |
| **代码**      | 1. **代码感知分割**：基于函数、类或逻辑块进行分割，而非任意字符数。LangChain 为多种编程语言提供了专用的分割器。 2. **使用专用 Embedding 模型**：选择在代码上训练过的模型，能更好地捕捉代码的句法和语义。 | 代码具有严格的语法结构，必须得到尊重。在函数或语句中间分割会使其失去意义。代码块的上下文通常由其作用域（函数或类）定义。 |
| **图片/图表** | 1. **图像描述/摘要**：使用多模态 LLM（如 GPT-4V, LLaVA）为图片或图表生成详细的文本描述，然后将这段描述文本进行向量化 。 2. **多模态 Embedding**：使用像 CLIP 这样的模型，它可以将图片和文本嵌入到同一个向量空间中，从而实现直接的“以文搜图”或“以图搜图” 。 | 这是**多模态 RAG** 的核心领域。选择生成描述还是使用多模态 Embedding 取决于具体应用。生成描述更灵活，因为产出的是标准文本，但可能丢失图像的细微差别；多模态 Embedding 更直接，但要求整个技术栈都能处理图像向量。 |



## 四、向量化：文字如何拥有“数学灵魂”

经过预处理，我们得到了一批高质量的知识单元。下一步，就是将这些文本转化为机器能够理解和比较的数学形式——这个过程就是**向量化（Vectorization）**，也常被称为**嵌入（Embedding）**。

### 1. 什么是 Embedding？

向量化，就是利用一个深度学习模型（即 Embedding 模型），将一段文本转换成一个由数百甚至数千个数字组成的列表，这个列表就是**向量（Vector）**。 打个比喻：Embedding 就像是为每一段文字分配了一个独特的“语义 GPS 坐标”。在由这些坐标构成的高维空间中，意思相近的文本，它们的坐标点在空间中的距离也会非常接近。例如，“苹果公司的最新财报”和“iPhone 制造商的季度收入”这两段话，它们的向量就会靠得很近。正是这种“语义相近，空间相邻”的特性，使得基于向量的**语义搜索 **成为可能，这也是 RAG 系统能够从海量知识中快速找到最相关信息的核心能力。

### 2. 向量模型怎么选？

选择一个合适的 Embedding 模型，是决定 RAG 检索效果的关键决策。这通常需要在开源与商业、性能与成本之间做出权衡。

• **开源 vs. 商业 API**

  对于 MTEB 排行榜，一个常见的误区是直接选择**平均分最高的模型** 。然而，对于 RAG 应用来说，`Retrieval`（检索）这项任务的得分才是最需要关注的指标。一个模型可能因为在分类、聚类等任务上表现优异而总分很高，但其检索能力可能平平。**一个优秀的开发者会优先筛选出在检索任务上表现优异的模型**，因为这直接关系到能否为 LLM 找到最优质的上下文。

![MTEB Taxonomy](RAG学习.assets\640-1756262403630-9.png)

- **开源模型 (如 BGE, GTE, E5 系列)**

- - **优点**：完全控制权，数据无需离开私有环境，保障了数据隐私；在拥有计算资源的情况下，大规模使用时成本可能更低；可以在自有数据上进行微调以提升特定领域的表现。
  - **缺点**：需要自行部署、管理和优化模型服务，这对技术团队有一定要求。
  - **权威参考**：**Hugging Face MTEB (Massive Text Embedding Benchmark) 排行榜**是业界公认的、用于比较各类开源 Embedding 模型性能的黄金标准。

- **商业 API (如 OpenAI, Cohere)**

- - **优点**：通常提供顶级的性能；使用极其简单，只需一个 API 调用；无需管理任何基础设施，服务稳定可靠。
  - **缺点**：按 Token 计费的模式在用量大时会变得昂贵；需要将数据发送给第三方服务商。

- **维度的权衡：越大越好吗？**

  向量维度（如 384, 768, 1024）指的是构成一个向量的数字个数。维度的选择是一个典型的权衡：

  - **高维度**：通常能捕捉到更丰富、更细微的语义信息，可能带来更高的检索精度。但代价是，它们需要更大的存储空间、更多的计算资源来进行相似度比较，从而导致更高的成本和延迟。
  - **低维度**：存储和计算成本更低，检索速度更快。但可能无法完全捕捉文本的复杂语义，有损失精度的风险。

在工程化落地过程中的策略往往是，<font color='red'>**从一个中等且性价比高的维度（如 768）开始，只有当评估发现检索质量成为瓶颈时，再考虑增加维度。**</font>

**表2：主流 Embedding 模型实用对比**

为了帮助开发者在繁多的模型中做出选择，下表对当前最受欢迎的几个模型系列进行了实用性对比。

| 模型系列                          | 提供方/来源                   | 核心优势                                                     | 成本模型/投入        | 典型维度                   |
| --------------------------------- | ----------------------------- | ------------------------------------------------------------ | -------------------- | -------------------------- |
| **text-embedding-3-small/large**  | OpenAI                        | 顶尖性能，API 简洁，支持自定义维度以平衡成本与性能。         | API 按 Token 计费    | 1536 (small), 3072 (large) |
| **Embed v3**                      | Cohere                        | 专为检索任务优化，支持多语言，提供不同类型的模型（如 `english`, `multilingual`）。 | API 按 Token 计费    | 1024                       |
| **BGE (BAAI General Embedding)**  | 北京智源人工智能研究院 (BAAI) | 在 MTEB 检索任务上持续名列前茅，中英文效果俱佳，完全开源。   | 自行部署（计算成本） | 1024                       |
| **GTE (General Text Embeddings)** | 阿里巴巴达摩院                | 强大的开源模型，性能与模型大小均衡，常被用作高质量的基线模型。 | 自行部署（计算成本） | 768 / 1024                 |



## 五、入库：为知识安“家”

当所有知识单元都被转换成向量后，我们需要一个地方来妥善地存储和管理它们，并支持高效的查询。这个“家”，就是**向量数据库（Vector Database）**。

### 1. 向量数据库：海量向量的“超级管家”

向量数据库是一种专门为存储、索引和查询海量高维向量而设计的数据库系统。它的核心能力是执行极其快速的 **近似最近邻（Approximate Nearest Neighbor, ANN）搜索**。

当一个用户问题被转换成查询向量后，如果要在数百万甚至数十亿的知识向量中找到最相似的几个，通过逐一比较（精确搜索）的方式是完全不可行的，会耗费巨大的计算资源和时间。`ANN` 搜索则通过构建巧妙的索引结构（如 `HNSW、IVF-PQ` 等算法），能够在牺牲极小的精度的前提下，以毫秒级的速度找到“足够好”的近似匹配结果。这使得实时语义搜索成为可能。

### 2. 主流选择对比

![How to Get the Right Vector Embeddings - Milvus Blog](RAG学习.assets\640-1756262610500-12.png)

市面上有多种向量数据库可供选择，选型时需考虑：

-  **可扩展性（Scalability）**：系统是否需要支持数十亿甚至更多的向量？对于大规模企业应用，需要选择像 `Milvus、Pinecone` 这样专为海量数据设计的数据库。
- **易用性与开发体验**：对于快速原型开发或中小型项目，Chroma 这样 API 简洁、易于上手的数据库可能更合适。
- **部署模式**：是选择 `Pinecone` 这样的全托管云服务（SaaS），省去运维烦恼，还是选择 `Milvus、Weaviate、Chroma` 这样的开源方案，进行私有化部署以获得更大的灵活性和数据控制权？。
- **高级功能**：是否支持**元数据过滤**（例如，只在“2023 年的财报”中搜索）或**混合搜索**（`Hybrid Search`，即结合传统的关键词搜索和向量语义搜索）？这些功能可以显著提升复杂查询的准确率。

其中 `Milvus、FAISS 和 Weaviate` 是最具代表性的三个。

- **Milvus**
  - **定位**：一个为大规模生产环境设计的高性能、高可用的开源向量数据库。它采用分布式架构，可以轻松扩展以支持数十亿级别的向量数据，并提供高吞吐量的查询服务。
  - **最适用场景**：大型企业级应用，对性能、可扩展性和系统稳定性有极高要求。可以把它看作是向量数据库领域的“重型工业引擎”。
- **FAISS (Facebook AI Similarity Search)**
  - **定位**：它本质上是一个**库（Library）**，而不是一个完整的数据库。FAISS 提供了业界最高效的 ANN 算法实现，但它不负责数据的存储、持久化、服务化等数据库管理功能。开发者需要自己围绕这个库来构建服务。
  - **最适用场景**：学术研究、算法原型验证，或者那些希望对底层拥有最大控制权、并愿意自行构建基础设施的专家团队。它的 **“轻量级”和“研究友好”** 体现在它专注于核心算法，没有数据库系统的额外开销。
- **Weaviate**
  - **定位**：一个功能丰富的开源向量数据库，提供了更多开箱即用的能力。它强调定义清晰的数据 Schema，并且原生支持 **混合搜索（Hybrid Search）**，即同时结合传统的关键字搜索和向量语义搜索，以提升检索的全面性。
  - **最适用场景**：需要将结构化元数据过滤与向量搜索深度结合的应用，或是希望快速实现强大的混合搜索能力而无需自行搭建的场景。它像是“功能完备、自带电池”的解决方案。

这三者的选择，不仅仅是技术特性的比较，更深层次地反映了项目所处的阶段和团队的开发哲学。选择 FAISS 意味着“自己动手，丰衣足食”，适合研究和原型阶段；选择 Weaviate 意味着追求“全面的解决方案”，适合功能丰富的 MVP 或中型应用；而选择 Milvus 则代表着“为海量规模而生”，是超大规模生产部署的终极选择。



| 数据库 (Database) | 类型 (Type) | 核心特性 (Key Features)                                      | 适用场景 (Ideal For)                                         | 易用性 (Ease of Use) |
| ----------------- | ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------- |
| **Pinecone**      | 托管云服务  | 高性能、高可用、全托管、混合搜索                             | 需要企业级稳定性和可扩展性，希望减少运维成本的生产环境       | ★★★★★                |
| **Milvus**        | 开源/云服务 | 专为大规模 AI 设计，高度可扩展，支持多种索引                 | 需要处理海量向量数据、对系统有深度定制需求的大型企业         | ★★★☆☆                |
| **Chroma**        | 开源/云服务 | 专为 RAG 设计，API 极其简单，与 LangChain 等框架深度集成     | 快速原型验证、中小型项目、研究和个人开发者                   | ★★★★★                |
| **Weaviate**      | 开源/云服务 | 内置向量化模块，支持多模态数据，提供 GraphQL API             | 需要处理文本、图片等混合数据，对数据结构化管理有要求的应用   | ★★★★☆                |
| **Elasticsearch** | 开源/云服务 | 强大的全文检索 (BM25) 与向量搜索结合，支持复杂的元数据过滤和聚合，适合混合搜索场景 | 已有 Elasticsearch 技术栈，希望在其上扩展向量检索能力的企业；需要将传统关键词搜索与语义搜索深度结合的应用 | ★★★☆☆                |
| **Redis**         | 开源/云服务 | 基于内存，提供极低的查询延迟；不仅是向量库，还可作为 RAG 流程中的语义缓存和会话历史存储，一物多用 | 对响应速度要求极高的实时应用；希望简化技术栈，用一个工具处理缓存和向量检索的中小型项目 | ★★★★☆                |

## 六、总结

本篇从五花八门的原始文档（如 PDF、网页）出发，通过精细的清洗和策略性的切分，将其转化为一个个蕴含上下文的知识片段。接着，我们借助强大的 **Embedding** 模型，为每个片段赋予了独特的“数学灵魂”——向量。最后，这些向量被妥善地安置在专为高速检索设计的向量数据库中，静静等待着被唤醒，为 LLM 提供精准的知识弹药。

知识库已经建好，但真正的 RAG 才刚刚开始。当用户提出一个问题时，系统是如何从数百万个向量中，瞬间找到最相关的那几个？“检索”和“重排”之间有什么区别？LLM 最终又是如何将这些零散的知识片段，融合成一段通顺、准确、令人信服的回答？



## 七、向量检索

当所有知识 `Chunk` 都被转换成向量后，我们需要一个地方来存储并高效地检索它们。这就是 **向量数据库（Vector Database）**的用武之地。它就像我们 “开卷考试” 中那座分门别类、索引清晰的图书馆

<img src="RAG学习.assets\640-1756273656983-28.jpeg" alt="vector-space-model" style="zoom:67%;" />

当用户提问时，`RAG` 系统会将问题同样用 `Embedding` 模型转换成一个查询向量（`Query Vector`），然后在向量数据库中执行相似性搜索，快速找出与查询向量“距离”最近的 `Top-K` 个文档向量，从而找到最相关的知识 `Chunk` 

### 1.引言

#### 1.1 检索的演进：从关键词到语义

**信息检索**（`Information Retrieval, IR`）的历史是一部不断追求更精准、更智能地匹配用户意图的演进史。早期的检索系统严重依赖于<font color='red'>**关键词匹配**</font>，即**词法搜索**（`Lexical Search`）。这种方法通过寻找查询词在文档中的精确出现来判断相关性，虽然高效，但其局限性也显而易见：它无法理解同义词、多义词或更深层次的语义关联。随着深度学习，特别是自然语言处理（`NLP`）技术的发展，我们进入了<font color='red'>**语义搜索**</font>（`Semantic Search`）的时代。语义搜索的核心思想是理解查询和文档背后的意义，而非仅仅是字面上的字符串。通过将文本、图像甚至音频等非结构化数据转换为高维向量（即嵌入，`Embeddings`），我们可以在一个数学空间中捕捉它们的语义关系，使得“动物”和“猫”在空间中彼此靠近。

#### 1.2 向量检索

本篇文章目的是为大家提供一份关于向量检索的详尽介绍，系统性地梳理从经典理论到前沿实践的完整知识图谱。我将从信息检索的基石——以`BM25`为代表的稀疏检索方法出发，为后续的讨论奠定理论基础。随后，我将深入探讨语义革命的核心——稠密向量检索，详细解析其背后的近似最近邻（`ANN`）搜索算法，特别是 `HNSW 和 IVF`。在此基础上，我将介绍混合检索与多级检索等高级策略，它们是构建现代高性能检索系统的关键。最后，我会介绍如何通过 `Embedding` 调优、查询优化和引入反馈机制等手段，极限提升召回精度，并展望向量检索技术的未来发展趋势。无论是刚接触 `RAG` 的开发者，还是寻求深度优化的资深工程师，希望在阅读完本篇之后能够对向量检索整个技术领域有个完整的理解。

### 2. 稀疏检索

在深入探讨现代向量检索之前，我们必须首先理解其前身：**稀疏检索方法**。这些基于关键词统计的经典算法，至今仍在许多高性能系统中扮演着不可或缺的角色。它们之所以被称为 **稀疏(`sparse`)**，是因为其生成的向量表示维度极高（通常与词汇表大小相当），但其中绝大多数元素为零。

#### 2.1 词频-逆文档频率（TF-IDF）

`TF-IDF` 是信息检索领域最具里程碑意义的算法之一。它的核心思想是：**一个词在一个文档中的重要性，与其在该文档中出现的频率成正比，但与其在整个语料库中出现的频率成反比**。这个简单的原则能够有效地过滤掉像“的”、“是”这类普遍存在但信息量低的“停用词”，同时提升那些能代表文档语义的关键词的权重。

![图片](RAG学习.assets\640-1756263214325-15.jpeg)

**核心原理**

TF-IDF的分数是通过将 **词频（Term Frequency, TF）** 与 **逆文档频率（Inverse Document Frequency, IDF）** 相乘得出的。

- **词频 (Term Frequency, TF)**：衡量一个词在特定文档中出现的频繁程度。最常见的计算方式是归一化频率，即用一个词在文档中出现的次数除以该文档的总词数，这样做可以避免对长文档的偏袒。
- **逆文档频率 (Inverse Document Frequency, IDF)**：衡量一个词的稀有度或信息量。一个词在越多的文档中出现，其IDF值就越低，说明它的区分能力越弱。其计算方法通常涉及取一个对数，对数内的数值是语料库中的总文档数除以包含该词的文档数。

**实践案例与局限性**

这里通过一个简单的例子来理解`TF-IDF`的计算过程。假设我们有两份文档：

- 文档A: "The car is driven on the road"
- 文档B: "The truck is driven on the highway"

对于词 `"car"` 在文档 A 中的 `TF-IDF` 分数，计算过程如下：

- **词频 (TF)**: `"car"` 在文档A中出现了一次，所以它的词频分数相对较高。
- **逆文档频率 (IDF)**: 在我们的两份文档中，`"car"` 只出现在一份文档里，说明它比较独特。因此，它的逆文档频率分数是一个正数。
- **最终TF-IDF分数**: 将两者相乘，`"car"` 获得了一个正向的权重分数，表明它对于文档A是重要的。

对于词 "the" 在文档A中的TF-IDF分数：

- **词频 (TF)**: `"the"` 同样在文档A中出现了一次。
- **逆文档频率 (IDF)**: `"the"` 在两份文档中都出现了，说明它是一个非常常见的词。因此，它的逆文档频率分数计算结果为零。
- **最终TF-IDF分数**: 任何数乘以零都等于零，所以 `"the"` 的最终`TF-IDF`分数为零，表明这个词信息量低。

尽管TF-IDF非常有效，但其局限性也十分明显：

1. **忽略语义信息**：它将词语视为独立的单元，无法理解 `"car"` 和 `"automobile"` 之间的语义相似性。
2. **忽略词序**：作为一个词袋模型，它丢失了词语的顺序信息，因此无法区分 `"not pay the bill"` 和 `"pay the bill"` 这样含义截然相反的短语。
3. **偏向长文档**：尽管 TF 的归一化在一定程度上缓解了这个问题，但长文档仍然有更多机会包含更多种类的词，从而可能获得不公平的高分。

#### 2.2 Okapi BM25

为了克服`TF-IDF`的诸多限制，大佬们开发了`Okapi BM25（Best Matching 25）`算法。`BM25`不仅仅是`TF-IDF`的一个简单改进，它代表了从启发式的向量空间模型到更严谨的概率信息检索模型的范式转变。它在两个核心方面对`TF-IDF`进行了根本性的优化。

![图片](RAG学习.assets\640-1756263612447-18.png)

- **词频饱和度 (Term Frequency Saturation)**：`TF-IDF`模型假设词频对相关性的贡献是线性增长的。然而，`BM25` 认识到一个更符合直觉的现实：当一个词在文档中出现的次数达到一定程度后，其对相关性的贡献会逐渐饱和，即收益递减。一份包含 200 次“大象”的文档，其相关性并不必然是一份包含100次“大象”的文档的两倍。
- **文档长度归一化 (Document Length Normalization)**：`TF-IDF` 缺乏一个内置的、有效的文档长度归一化机制。`BM25` 则明确地将文档长度与其在整个语料库中的平均长度进行比较，并据此对分数进行调整，从而更公平地惩罚那些仅仅因为篇幅长而导致词频较高的文档 。

**BM25的评分机制与参数**

`BM25` 通过一个综合性的评分机制来评估文档与查询的相关性，该机制由可调参数 `k1 和 b` 控制：

- **k1**：词频饱和度参数。它决定了词频分数增长曲线的陡峭程度。`k1` 的值通常设置在`1.2` 到 `2.0` 之间。一个较高的 `k1` 值会延迟饱和点的到来，使得高频词获得更高的权重。
- **b**：文档长度归一化参数。它的值域在 `0` 到 `1` 之间，通常取 `0.75`。当 `b` 设为`1`时，系统会应用完全的长度归一化；当设为`0`时，则完全不考虑文档长度的影响。

**为何 BM25 成为标准**

凭借其在词频饱和度和文档长度归一化方面的精妙设计，以及可调参数带来的灵活性，`BM25` 在各种信息检索任务中都表现出比`TF-IDF`更优越且更稳定的性能。它在简单性和性能之间取得了出色的平衡，使其成为现代搜索引擎（如 Elasticsearch）和向量数据库（如Milvus）中默认的稀疏检索算法，构成了许多复杂检索系统（包括混合检索）的基础。

**表1：TF-IDF vs. BM25：一项对比分析**

| 特性               | TF-IDF                             | BM25                                   |
| :----------------- | :--------------------------------- | :------------------------------------- |
| **理论模型**       | 向量空间模型 (Vector Space Model)  | 概率信息检索模型 (Probabilistic Model) |
| **词频处理**       | 线性或对数增长，无上限             | 非线性增长，具有饱和效应               |
| **文档长度归一化** | 无内置机制，通常需要外部归一化     | 内置归一化，通过参数 `b` 调节          |
| **核心参数**       | 无                                 | k1 (饱和度), b (长度归一化)            |
| **计算成本**       | 较低                               | 略高                                   |
| **典型用例**       | 基础的关键词权重计算、文本分析入门 | 现代搜索引擎、RAG系统中的稀疏检索标准  |



### 3. 稠密向量检索

随着深度学习的兴起，信息检索领域经历了一场从词法到语义的深刻革命。这场革命的核心，是将文本、图像等非结构化数据转化为能够捕捉其深层含义的稠密向量（`Embeddings`），并在此基础上进行高效的相似性搜索。

#### 3.1 从词语到向量：Embedding

与`TF-IDF`或`BM25`生成的稀疏向量不同，Embedding 是通过神经网络模型（如BERT或Sentence-BERT）学习到的低维、稠密的实数向量。这些向量的每一个维度并不直接对应某个特定的词，而是共同编码了输入数据的复杂语义特征。这种表示方式的巨大优势在于，语义上相近的内容，即使使用的词语完全不同，其对应的向量在向量空间中的位置也会非常接近 。例如，“夏季服装”和“短裤与T恤”的向量会比它们和“冬季大衣”的向量更近。这使得计算机能够超越关键词匹配，实现真正意义上的“理解” 。

然而，这种强大的表示能力也带来了新的挑战：**我们如何在包含数百万甚至数十亿个高维向量的数据库中，快速找到与给定查询向量最相似的几个向量**？传统的精确最近邻（k-NN）搜索算法需要计算查询向量与数据库中每一个向量的距离，在海量数据面前，这种暴力搜索的计算成本是无法接受的 。

![image-20250827111626822](RAG学习.assets\image-20250827111626822-1756264589944-21.png)

#### 3.2 近似最近邻（ANN）搜索

为了解决规模化搜索的难题，近似最近邻（`Approximate Nearest Neighbor, ANN`）搜索应运而生。`ANN` 的核心思想是一种工程上的权衡：放弃寻找绝对精确的最近邻，以换取搜索速度的指数级提升。对于绝大多数应用场景，如**推荐系统或语义搜索**，找到“足够相似”的结果已经完全满足需求，而牺牲的微小精度损失通常可以忽略不计。`ANN` 算法通过构建巧妙的数据结构（索引），在搜索时能够跳过大量不相关的向量，从而避免了全局的暴力比较 。

**从空间划分到关系导航的理念飞跃**

ANN算法的发展历程体现了一种核心理念的转变，即从试图对整个高维空间进行“划分”转变为在数据点之间建立“导航”网络。这一转变是应对 **维度灾难** 的根本性策略。

早期的`ANN`算法，如`KD`树，其工作原理是通过递归地用超平面将向量空间进行划分。这种方法在低维空间中非常直观有效。然而，随着维度的增加，空间的体积会呈指数级增长，数据点变得极其稀疏，“附近”这个概念也变得模糊不清。

相比之下，现代的基于图的`ANN`算法，如`HNSW`，采取了截然不同的策略。它不再试图去理解和划分整个宏观空间，而是在数据点之间构建一个连接网络（图），其中边代表了数据点之间的邻近关系。搜索过程不再是遍历一个刚性的树状结构，而是在这个关系网络中，从一个入口点开始，沿着边“导航”到查询向量的最近邻。这种方法更加鲁棒，因为它依赖的是数据本身的局部邻域结构，而这种结构即使在高维空间中也依然存在。

**表2：ANN 算法版图：HNSW vs. IVF**

在众多`ANN`算法中，基于图的`HNSW`和基于聚类的`IVF`是当今向量数据库中最主流的两种索引策略。它们代表了两种不同的`ANN`实现哲学：`HNSW` 侧重于构建可导航的邻近图，而`IVF`则通过聚类来裁剪搜索空间。

| 维度             | HNSW (Hierarchical Navigable Small World) | IVF (Inverted File)                      |
| :--------------- | :---------------------------------------- | :--------------------------------------- |
| **底层算法**     | 基于图的导航                              | 基于聚类的划分                           |
| **数据结构**     | 多层邻近图                                | 倒排文件（聚类中心 -> 向量列表）         |
| **索引构建时间** | 较慢，需要构建复杂的图结构                | 较快，主要耗时在k-means聚类训练          |
| **查询速度**     | 极快，对数级别复杂度                      | 快，但速度依赖于 `nprobe` 参数           |
| **内存占用**     | 高，需要存储图的连接信息                  | 较低，主要存储向量和聚类中心             |
| **核心参数**     | `M`, `ef_construction`, `ef_search`       | `nlist`, `nprobe`                        |
| **优势**         | 查询速度快，召回率高，无需训练            | 内存效率高，索引构建快                   |
| **劣势**         | 内存消耗大，索引构建耗时                  | 召回率对 `nprobe` 敏感，可能错过边缘向量 |

#### 3.3 HNSW 解析

HNSW 即Hierarchical Navigable Small World graphs（分层-可导航-小世界-图）的缩写，可以说是在工业界影响力最大的基于图的近似最近邻搜索算法（Approximate Nearest Neighbor，ANN），没有之一。HNSW 是一种非常流行和强大的算法，具有超快的搜索速度和出色的召回率。它巧妙地结合了两种思想：可导航小世界图和概率跳表。

- **算法原理**：`HNSW` 构建了一个分层的图结构。顶层图最稀疏，连接最长，像“高速公路”，负责全局的快速导航；底层图最密集，包含了所有向量，像“本地道路”，负责局部的精确搜索。

  <img src="RAG学习.assets\image-20250827112245661-1756264967726-23.png" alt="image-20250827112245661" style="zoom:67%;" />

- **构建与搜索过程**：

  - **构建 (Construction)**：向量被逐一插入图中。算法从顶层图开始，为新向量在每一层都寻找最近的邻居，并根据参数 `M`（每个节点的最大连接数）和 `ef_construction`（构建时的搜索范围大小）建立连接。
  - **搜索 (Search)**：搜索同样从顶层的某个入口点开始。在当前层，算法沿着图的边进行**贪婪搜索**，不断走向离查询向量更近的节点。然后，算法将这个局部最优解作为下一层的入口点，重复此过程，逐层向下，直到在最底层找到最终的近似最近邻。

  

#### 3.4 IVF 解析

`IVF` 系列索引是另一种广泛使用的 `ANN` 技术，其核心思想借鉴了传统文本检索中的**倒排索引**，通过聚类来减少需要比较的向量数量。

- **算法原理**：
  1. **训练 (Training)**：在构建索引之前，算法（通常是`k-means`）将整个向量空间划分为 `nlist` 个聚类（或称为“桶”）。每个聚类的中心点被称为“质心”（Centroid）。
  2. **索引 (Indexing)**：训练完成后，将数据集中的每一个向量分配到其最近的质心所对应的聚类中。
  3. **查询 (Querying)**：当一个查询向量到来时，系统首先找出最近的 `nprobe` 个质心。然后，搜索范围被限定在这 `nprobe` 个聚类所包含的向量中，从而大大减少了计算量。
- **IVF-PQ变体**：为了进一步优化内存占用和查询速度，IVF经常与**乘积量化（Product Quantization, PQ）**结合使用。PQ是一种向量压缩技术，它将一个高维向量切分成多个低维的子向量，然后对每个子向量空间分别进行量化，极大地降低了存储需求。

![图片](RAG学习.assets\640-1756265062505-25.png)

### 4. 高级检索策略

在掌握了单一的稀疏或稠密检索方法后，构建一个真正强大的、生产级的检索系统还需要更高级的架构设计。这些策略通过组合不同的技术，取长补短，以应对真实世界中复杂多变的查询需求。

#### 4.1 混合检索

混合检索（`Hybrid Search`）是一种将**关键词检索**（稀疏）与**语义检索**（稠密）相结合的强大策略，旨在同时利用两者的优势，提供更精准、更鲁棒的搜索结果。

**混合检索**

纯粹的语义检索虽然在理解用户意图方面表现出色，但在处理需要精确关键词匹配的场景时却常常力不从心。例如，用户搜索一个特定的产品型号（如“SKU-12345”）、一个专有品牌名或一个技术缩写时，向量模型可能无法准确捕捉其独特性。这恰恰是稀疏检索（以`BM25`为代表）的强项。因此，混合检索的出现并非锦上添花，而是一种务实的修正，它承认了在真实应用中，上下文理解和关键词精度同等重要。

**架构与结果融合**

一个典型的混合检索系统的工作流程如下：

- **并行查询**：当接收到用户查询后，系统会同时将其发送给稀疏检索管道（如`BM25`）和稠密检索管道（`ANN`搜索）。
- **结果融合**：两个管道各自返回一个排序后的文档列表。由于两者分数范围和分布截然不同，直接加权融合效果不佳。因此，业界普遍采用**倒数排名融合（Reciprocal Rank Fusion, RRF）**。RRF不关心每个结果的原始分数，只关心它在各自列表中的排名（`rank`）。一个结果在原始列表中排名越靠前，它在最终融合结果中的权重就越大。

#### 4.2 多级检索

多级检索，特别是 **检索-重排**（`Retrieve-and-Rerank`）架构，是另一种在精度和效率之间取得极致平衡的高级策略。它将检索过程分解为两个阶段：

- **第一阶段：召回（Retrieval）**： 目标是**高召回率（High Recall）**，此阶段的目标是从海量的文档库中，快速、广泛地筛选出一个较小的候选集（例如，前100个文档），确保尽可能多的相关文档被包含在内。通常使用速度快的模型，如基于`ANN`索引的向量检索（`Bi-encoder`）或 `BM25`。
- **第二阶段：重排（Reranking）**：目标是**高精确率（High Precision）**。此阶段接收来自第一阶段的候选集，并使用一个更强大、计算更密集的模型对这个小集合进行精细的重新排序。这个阶段的主要是**Cross-Encoder（交叉编码器）**模型。

**Cross-Encoder**

`Cross-Encoder`之所以在重排阶段表现卓越，其根本原因在于其独特的处理机制。与`Bi-Encoder`（双编码器）将查询和文档独立编码为向量再计算相似度不同，`Cross-Encoder` 将**查询和单个文档拼接成一个输入对**，然后将这个组合输入到一个完整的`Transformer`模型中进行处理 。

这种“共同处理”的方式允许模型内部的自注意力机制在查询和文档的每个词元之间进行深度、细粒度的交互建模，从而输出一个远比向量相似度更精准的相关性分数。然而，这种高精度是有代价的。`Cross-Encoder`的联合处理过程必须在查询时对每一个候选文档都执行一次完整的、耗时的前向传播，因此它无法被预计算，速度要慢得多。

正是这种“快而粗”的`Bi-Encoder`和“慢而精”的`Cross-Encoder`之间的特性互补，使得 **检索-重排** 范式成为构建顶级信息检索系统的黄金标准。



### 5. 优化与性能调优

构建一个向量检索系统不仅仅是选择算法和架构，更关键的是如何根据具体应用场景对系统进行细致的调优，以在**查询速度、召回精度和资源消耗**这三个维度上找到最佳平衡点。

#### 5.1 索引优化

索引是向量检索性能的核心。对索引参数的精细调整，是决定系统最终表现的关键一步。

**HNSW参数调优**

HNSW索引的性能主要由三个参数控制：

- **`m`**：定义了每个节点允许拥有的最大连接数。增加 `m` 会使图更密集，通常能提高召回率，但代价是内存占用和构建时间增加 35。
- **`ef_construction`**：索引构建时的搜索范围大小。一个更大的值可能构建出质量更高的图，有助于提升查询召回率，但这会显著增加索引构建时间。
- **`ef_search`**：查询时的搜索范围大小。这是在查询速度和召回率之间进行权衡的主要调节旋钮。增加 `ef_search` 会提高召回率，但也会直接导致查询延迟增加。

**IVF参数调优**

IVF系列索引的性能主要由两个参数控制 ：

- **`nlist`**：定义了向量空间被划分成的聚类数量。一个较大的 `nlist` 值意味着每个聚类中的向量数量较少，可以提高查询速度，但过多的聚类也可能降低召回率。经验法则是，对于百万级向量，`nlist` 可以设为 `4 * sqrt(向量总数)`。
- **`nprobe`**：查询时需要探查的聚类数量。这是直接控制查询速度和召回率之间平衡的关键。增加 `nprobe` 会提高召回率，但会线性增加查询的计算量和延迟。

**表3：HNSW与IVF索引调优参数**

| 索引类型 | 参数              | 描述                 | 增加该参数的影响                                             |
| :------- | :---------------- | :------------------- | :----------------------------------------------------------- |
| **HNSW** | `m`               | 每个节点的最大连接数 | **召回率 ↑**, **查询速度 ↓**, **索引大小 ↑**, **构建时间 ↑** |
|          | `ef_construction` | 构建索引时的搜索范围 | **召回率 ↑**, **查询速度 ↔**, **索引大小 ↔**, **构建时间 ↑** |
|          | `ef_search`       | 查询时的搜索范围     | **召回率 ↑**, **查询速度 ↓**, **索引大小 ↔**, **构建时间 ↔** |
| **IVF**  | `nlist`           | 聚类中心的数量       | **召回率 ↓**, **查询速度 ↑**, **索引大小 ↑**, **构建时间 ↑** |
|          | `nprobe`          | 查询时探查的聚类数量 | **召回率 ↑**, **查询速度 ↓**, **索引大小 ↔**, **构建时间 ↔** |

#### 5.2 查询优化

- **元数据过滤**：在许多应用中，向量搜索需要与传统的元数据过滤结合。**预过滤** 在向量搜索前应用过滤器，缩小搜索范围但可能降低`ANN`效率；**后过滤 **先执行向量搜索再过滤，更简单快速，但可能需要“过采样”以确保过滤后仍有足够结果。
- **降维与量化**：**维度约减**（如`PCA`）可以减少存储和计算成本，但可能损失精度。**量化**（如`PQ`）通过压缩向量来减少内存占用和加速计算，是在精度和资源消耗之间进行权衡的有效手段。

#### 5.3 高吞吐系统的缓存策略

对于查询量巨大的系统，引入缓存层是降低延迟、节省计算资源的关键。

- **查询与结果缓存**：将频繁出现的、完全相同的查询及其结果存储在高速缓存中（如 Redis）。
- **语义缓存**：这是一种更高级的策略，它不仅仅匹配精确的查询字符串，而是理解查询的“语义”。当一个新查询到来时，系统会将其嵌入为向量，并在缓存中搜索是否存在语义上足够相似的、已经处理过的查询。如果找到，系统就直接返回其对应的答案，这可以极大地减少对下游昂贵的`LLM API`的调用次数。
- **向量缓存**：将系统中被频繁访问的“热点”向量缓存在内存中，可以加速检索过程。

### 6. Milvus 中的索引技术

`Milvus` 提供了丰富多样的索引类型，以适应不同场景下的性能、精度和资源消耗需求。

**表4：Milvus 浮点向量索引类型**

| 索引类型      | 分类                      | 理想场景与权衡                                               |
| :------------ | :------------------------ | :----------------------------------------------------------- |
| **FLAT**      | 暴力搜索                  | 数据集规模较小（百万级以内），且要求100%精确召回率的场景。速度最慢。 |
| **IVF_FLAT**  | 基于量化（聚类）          | 追求高查询速度和尽可能高的召回率之间的平衡。                 |
| **IVF_SQ8**   | 基于量化（聚类+标量量化） | 对内存资源有限制，同时追求高查询速度，可以接受轻微召回率损失的场景。 |
| **IVF_PQ**    | 基于量化（聚类+乘积量化） | 内存资源非常有限，追求极高的查询速度，可以接受更明显召回率损失的场景。 |
| **HNSW**      | 基于图                    | 对查询速度和召回率有极高要求，且拥有充足内存资源的场景。     |
| **GPU_CAGRA** | GPU优化                   | 适用于需要利用GPU进行大规模并行计算加速的负载，以获得极致的查询性能。 |

#### 6.1 数据操作

`Milvus` 提供了一套完整的 API，用于对向量数据进行全生命周期的管理，支持标准的 `CRUD`（创建、读取、更新、删除）操作。所有的数据修改操作都会先写入日志代理，确保了操作的持久性和可恢复性。

在 Milvus 的 API 中，理解`search`和`query`的区别至关重要：

- **`search`**：特指**向量相似性搜索**。它的输入是一个或多个查询向量，输出是与之最相似的向量及其元数据。
- **`query`**：特指基于**标量字段的元数据过滤**。它的工作方式类似于传统SQL数据库中的`WHERE`子句，用于根据非向量字段的条件来精确检索实体。

在实际应用中，这两者经常结合使用，实现混合搜索，从而在满足特定标量条件的数据子集中进行高效的向量相似性搜索 。



### 7. 提升召回与精度

一个检索系统的构建，不仅依赖于底层的索引和架构，更取决于对系统“两端”——即输入端的 `Embedding` 模型和用户查询——的精细打磨。

#### 7.1 Embedding 模型调优

`Embedding` 模型是整个语义检索系统的基石，其质量直接决定了检索效果的天花板。

<font color='red'>**模型选择**</font>

**海量文本嵌入基准（Massive Text Embedding Benchmark, MTEB）**及其多语言版本（`MMTEB`）已成为业界公认的权威资源，它对数百个模型在多种任务和上千种语言上进行了全面评估。开发者应当优先考虑在MTEB上表现优异的、与自己应用场景相匹配的模型。

当前，`Embedding` 模型领域呈现出两大趋势：

1. **指令微调（Instruction-Tuned）模型**：如`multilingual-e5-large-instruct`等模型，通过在训练中加入任务指令，使得模型能够更好地理解任务意图。
2. **领域专用（Domain-Specific）模型**：通用模型在处理特定领域（如法律、医疗、金融）的专业术语时往往表现不佳。因此，针对特定领域数据进行微调的模型能够取得远超通用模型的效果。

<font color='red'>**领域微调**</font>

当现成的模型无法满足特定业务需求时，对基础模型进行领域专属的微调（Fine-tuning）就显得至关重要。

- **策略与损失函数**：微调通常采用 **对比学习（Contrastive Learning）**的范式，其核心思想是，在`Embedding`空间中，拉近“相似”样本对的距离，推远“不相似”样本对的距离。常用的损失函数包括 **Triplet Loss（三元组损失）**和更高效的 **Multiple Negatives Ranking Loss (MNR Loss)** 。
- **数据准备**：微调的成败很大程度上取决于训练数据的质量。特别是 “**困难负样本（Hard Negatives）**”，即那些与查询非常相关但实际上是错误答案的样本，`Hard Negatives` 对于训练出具有高区分度的模型非常重要。

<font color='red'>**评估 Embedding 质量**</font>

如何科学地衡量 Embedding 的质量？评估方法可分为 **内在评估**（评估向量本身性质）和 **外在评估**（将Embedding 应用于下游任务并衡量其性能）两大类。对 RAG 系统而言，外在评估最有实际意义。

**表5：检索任务核心评估指标**

| 指标                           | 定义                                                         | 解读                                                         | 适用场景                                                 |
| :----------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- | :------------------------------------------------------- |
| **Recall@k**                   | 在返回的前k个结果中，包含了多少比例的“所有”相关文档。        | 衡量系统的“查全率”或覆盖能力。                               | 文档检索、商品推荐等需要尽可能多地发现相关项的场景。     |
| **Mean Reciprocal Rank (MRR)** | 多个查询的倒数排名（第一个正确答案所在位置的倒数）的平均值。 | 衡量系统将第一个相关结果排在多靠前的位置。高分（接近1）表示系统总能把正确答案放在最前面。 | 问答系统、事实查找等用户期望快速找到唯一正确答案的场景。 |
| **Precision@k**                | 在返回的前k个结果中，有多少比例是相关的。                    | 衡量返回结果的“准确率”。高分表示返回的顶部结果相关性很高。   | 网页搜索等用户主要关注顶部结果质量的场景。               |
| **NDCG@k**                     | 归一化折损累计增益，同时考虑了结果的相关性等级和位置。       | 最全面的排名质量指标之一，奖励将更相关的结果排在更前面。     | 任何对结果排序质量有精细化要求的场景。                   |

#### 7.2 查询优化

用户输入的原始查询往往是模糊的，在检索前对查询进行优化也非常重要。

- **使用 LLM 进行查询重写与扩展**：大型语言模型强大的语言理解和生成能力使其成为优化查询的理想工具。具体方法包括 **查询重写**（将模糊查询改写为精确查询）、**查询扩展**（补充同义词或相关概念）和 **生成假设性文档**（让`LLM`生成一个“虚拟”的理想答案文档，并使用其 `Embedding` 进行搜索）。
- **传统查询扩展技术**：包括利用**同义词库**（如`WordNet`）进行扩展，以及经典的**伪相关反馈（Pseudo-Relevance Feedback, PRF）**技术，即利用初次检索结果中的高频词来扩展原始查询。

### 8. 负反馈

传统的 `RAG` 系统是静态的，通过引入用户反馈循环，特别是负反馈，我们可以将 `RAG` 系统从一个静态的信息处理器转变为一个能够持续学习和自我改进的动态系统。这一演进过程可以分为几个阶段：

- **引入反馈信号**：系统开始收集用户的显式反馈（如“赞”或“踩”）或隐式反馈（如点击行为）。负反馈信号尤其强大，因为它能明确地告诉模型哪些内容是“不应该”检索的，从而更有效地帮助模型划定决策边界。
- **离线模型迭代**：收集到的反馈数据可被用来定期地、离线地对`Embedding`模型或`Reranker`模型进行微调，创建了一个手动的改进循环。
- **在线自适应学习**：最终的进化形态是利用 **基于人类反馈的强化学习（Reinforcement Learning from Human Feedback, RLHF）**来构建一个闭环的、自适应的 RAG 系统。在这个框架中，用户的交互被视为环境提供的“奖励”信号。一个强化学习智能体可以学习一个策略，来优化其检索行为（例如，决定是否检索、检索多少文档），其目标是最大化长期累积的奖励。



### 9. 整体流程

现在，让我们把所有零件组装起来，跟随一个具体的提问，走完一次完整的 RAG 旅程。

![图片](RAG学习.assets\640-1756273896736-31.png)

> **场景**：一位产品经理向公司内部的 AI 助手提问：“我们最新发布的‘星尘’AI 芯片相比上一代‘光子’芯片，在能效和推理速度上具体有哪些提升？请给出数据。”

1. **提问（Query）**：用户在聊天框输入问题。
2. **向量化（Embedding）**：系统调用 Embedding 模型，将这个问题转换成一个 768 维的查询向量。
3. **召回（Recall）**：系统拿着这个查询向量，到存储着公司内部文档（产品白皮书、性能测试报告、发布会 PPT 等）的 Milvus 向量数据库中进行相似性搜索，召回 Top 50 的相关文本 Chunk。
4. **重排（Reranking）**：Reranker 模型（一个 Cross-Encoder）对这 50 个 Chunk 进行精细打分，发现其中 3 个 Chunk 的相关性得分最高：一块来自《“星尘”芯片性能基准测试报告.pdf》的表格数据，一块来自《Q3 产品发布会.pptx》中对比两代芯片的关键幻灯片文字，还有一块来自《“星尘”芯片技术规格书.docx》的能效章节。
5. **提示词构建（Prompt Augmentation）**：系统构造一个增强提示词，结构如下：

![image-20250827140045102](RAG学习.assets\image-20250827140045102-1756274446978-34.png)

6. **生成（Generation）**：LLM（如 Deepseek R1/Qwen）接收到这个结构清晰、信息充分的提示词，轻松地从中提取关键数据并组织成答案。

7. **回答（Answer）**：AI 助手最终输出一个精准、有据可查的回答： 

```
“根据公司的内部文档，‘星尘’AI 芯片相比‘光子’芯片在能效和推理速度上有显著提升：
**1、推理速度**：在 ResNet-50 模型上，‘星尘’芯片的推理速度为 2500 FPS，相比‘光子’芯片的 1800 FPS 提升了 38%（来源：性能基准测试报告.pdf）。
**2、能效**：‘星尘’芯片的典型功耗为 15W，相比‘光子’芯片的 25W，能效比提升了约 67%（来源：技术规格书.docx）。”
```

 至此，一次高质量的 RAG 问答流程圆满完成。

### 10. 总结

本文系统地梳理了向量检索技术从经典稀疏模型到现代稠密模型的演进脉络，深入探讨了从底层算法、高级架构到系统优化的全栈知识体系。

**核心要点回顾**

- **稀疏与稠密的协同**：`BM25` 的关键词精度与向量模型的语义理解能力高度互补。混合检索已成为业界共识，是构建鲁棒、高精度检索系统的基石。
- **多级架构的必要性**：面对效率与精度的两难，"检索-重排"范式提供了一个优雅的工程解决方案，通过快速召回和精确重排实现高性能检索。
- **Embedding模型的中心地位**：检索系统的上限由 `Embedding` 模型的质量决定。模型选择、领域微调以及科学的评估是提升系统性能的根本途径。
- **端到端的系统优化**：一个生产级的检索系统是算法、架构、工程和数据协同作用的结果。从索引参数调优、查询理解到缓存策略，每一个环节的优化都至关重要。

**一些思考**

向量检索技术的发展已进入深水区。构建下一代人工智能应用，要求我们必须采取一种系统性的、端到端的视角。这不仅意味着要关注单个算法的创新，更要着眼于如何将数据、模型、架构和用户反馈融合成一个高效、智能、可持续演进的闭环生态系统。



## 八、RAG 的“进化”与“变体”：不止于简单的问答

基础的 RAG 框架已经非常强大，但它仅仅是一个起点。整个领域正在飞速发展，涌现出许多更智能、更强大的 RAG 变体。了解这些前沿方向，有助于我们思考 AI 应用的未来形态。这种演进轨迹，也正反映了 AI 系统从固定的、基于规则的流水线，向着动态的、具备推理能力的、能够使用多种工具的智能体方向发展的宏大趋势。

![image-20250720000052967](RAG学习.assets\640-1756274740567-36.png)



• **缓存增强生成 (Cache-Augmented Generation, CAG)**：随着 `LLM` 的上下文窗口越来越大（动辄百万 `Token`），一种 `RAG` 的替代方案 `CAG` 开始受到关注。对于那些相对静态、大小可控的知识库，`CAG` 会在系统启动时就将所有知识预加载到模型的 KV 缓存中。这样，在响应用户查询时，就完全省去了实时检索的延迟，响应速度极快。但它的缺点是灵活性较差，不适用于频繁更新的动态知识库。

![img](RAG学习.assets\https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Facf8b099-7e87-4f81-9b0a-c68eb225fbec_1100x1285.jpeg)

Let’s discuss them briefly:

**1)  Naive RAG**

- Retrieves documents purely based on vector similarity between the query embedding and stored embeddings.

  纯粹基于查询嵌入和存储嵌入之间的向量相似性来检索文档。

- Works best for simple, fact-based queries where direct semantic matching suffices.

  最适合简单的、基于事实的查询，其中直接语义匹配就足够了。

**2）Multimodal RAG  多模态RAG**

- Handles multiple data types (text, images, audio, etc.) by embedding and retrieving across modalities.

  通过跨模态嵌入和检索来处理多种数据类型（文本、图像、音频等）。

- Ideal for cross-modal retrieval tasks like answering a text query with both text and image context.

  非常适合跨模式检索任务，例如使用文本和图像上下文回答文本查询

**3) HyDE**

- Queries are not semantically similar to documents.

  查询在语义上与文档并不相似。

- This technique generates a hypothetical answer document from the query before retrieval.

  查询在语义上与文档并不相似。

- Uses this generated document’s embedding to find more relevant real documents.

  使用此生成的文档的嵌入来查找更多相关的真实文档。

**4) Corrective RAG**

- Validates retrieved results by comparing them against trusted sources (e.g., web search).

  通过将检索到的结果与可信来源（例如，网络搜索）进行比较来验证检索到的结果。

- Ensures up-to-date and accurate information, filtering or correcting retrieved content before passing to the LLM.

  确保信息是最新且准确的，在传递给 LLM 之前过滤或更正检索到的内容。

- **自纠正 RAG (Self-Correcting RAG)**：这类 `RAG` 引入了“反思”和“修正”机制。例如，**CRAG (Corrective-RAG)** 在检索后增加了一个评估器（Evaluator）来判断检索到的文档质量。如果文档不相关，它会自动触发 Web 搜索来补充信息，或者改写查询词重新检索。而 **Self-RAG** 更进一步，它训练 LLM 自身学会生成特殊的“反思令牌”，让模型自己判断是否需要检索、检索回来的内容是否有用、自己生成的答案是否基于了证据，从而实现全流程的自我优化和批判。

**5) Graph RAG 图RAG**

- Converts retrieved content into a knowledge graph to capture relationships and entities.

  将检索到的内容转换为知识图以捕获关系和实体。

- Enhances reasoning by providing structured context alongside raw text to the LLM.

  通过向 LLM 提供结构化上下文和原始文本来增强推理能力

- **图 RAG (Graph-RAG)**：当知识本身具有很强的关联性时（比如组织架构、产品依赖关系、知识图谱），传统的文本块检索就显得力不从心。`Graph-RAG` 将知识库构建成一个知识图谱，检索时不再是返回孤立的文本块，而是返回相互连接的实体及其关系。这为 LLM 提供了更深层次的结构化上下文，使其能够进行更复杂的推理。

**6）Hybrid RAG 混合RAG**

- Combines dense vector retrieval with graph-based retrieval in a single pipeline.

  在单个管道中将密集矢量检索与基于图的检索相结合

- Useful when the task requires both unstructured text and structured relational data for richer answers.

  当任务需要非结构化文本和结构化关系数据来获得更丰富的答案时很有用。

**7）Adaptive RAG 自适应RAG**

- Dynamically decides if a query requires a simple direct retrieval or a multi-step reasoning chain.

  动态地决定查询是否需要简单的直接检索或多步骤推理链。

- Breaks complex queries into smaller sub-queries for better coverage and accuracy.

**8） Agentic RAG**

- Uses AI agents with planning, reasoning (ReAct, CoT), and memory to orchestrate retrieval from multiple sources.

- Best suited for complex workflows that require tool use, external APIs, or combining multiple RAG techniques.

- **智能体 RAG (Agentic RAG)**：这是 `RAG` 演进中最具颠覆性的一步。它不再是一个固定的“检索-生成”流水线，而是由一个 LLM 驱动的 **智能体（Agent）** 来主导整个过程。这个智能体具备规划和使用工具的能力。当收到一个复杂问题时，它会先进行“思考”（Thought），然后规划出解决问题的步骤（Plan），并决定调用哪个“工具”（Tool）来执行。这些工具可能包括：

  Agentic RAG 将 RAG 从一个信息检索框架，提升为了一个动态的、多才多艺的问题解决框架。RAG 本身，也从整个系统，降级成了智能体可以选用的一种工具。

  - 进行向量检索（标准的 `RAG` 步骤）
  - 执行 `SQL` 查询从结构化数据库中获取数据
  - 调用外部 `API` 获取实时信息（如天气、股价）
  - 使用计算器进行数学运算



## 九、总结

回顾全文，我们可以清晰地看到，RAG 并非一个高深莫测的算法，而是一种极其务实且强大的工程思想。它直面了通用大模型在落地应用时最核心的三个痛点：**知识局限、事实幻觉和私域无知**。

通过将 LLM 的通用推理能力与企业外部或内部的特定知识源相结合，RAG 成功地为模型装上了“事实的锚”，使其回答既能保持语言的流畅自然，又能做到内容的准确可靠。

对于任何希望利用大模型技术创造价值的企业而言，RAG 都是那把不可或缺的“金钥匙”。它是一座至关重要的桥梁，连接了公域的通用语言智能与私域的、构成企业核心竞争力的专有数据。掌握 RAG，不仅仅是学会一项技术，更是理解并采纳一种全新的、可持续的 AI 应用构建范式。对于每一位致力于用 AI 打造可靠、可信、可扩展产品的产品经理、开发者和架构师来说，这条路，才刚刚开始。